{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Powered Interview Question Generator & Evaluator (LangChain + Azure OpenAI)\n",
        "\n",
        "This notebook demonstrates a **multi-agent** interview system built with **LangChain** and **Azure OpenAI**. It:\n",
        "1) analyzes a job description,\n",
        "2) generates role-specific interview questions,\n",
        "3) evaluates candidate answers, and\n",
        "4) produces a structured hiring recommendation using an orchestrated agent workflow with session memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install -U langchain-openai langchain-core langgraph python-dotenv pydantic azure-storage-blob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configure Azure OpenAI\n",
        "\n",
        "Set these environment variables (recommended: `.env`).\n",
        "\n",
        "- `AZURE_OPENAI_ENDPOINT` e.g. `https://<resource>.openai.azure.com/`\n",
        "- `AZURE_OPENAI_API_KEY`\n",
        "- `OPENAI_API_VERSION`\n",
        "- `AZURE_OPENAI_CHAT_DEPLOYMENT`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "required = [\"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_API_KEY\", \"OPENAI_API_VERSION\", \"AZURE_OPENAI_CHAT_DEPLOYMENT\"]\n",
        "missing = [k for k in required if not os.getenv(k)]\n",
        "print(\"Missing env vars:\", missing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define schemas (structured outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Literal, Optional\n",
        "\n",
        "class JobAnalysis(BaseModel):\n",
        "    role_title: str = Field(..., description=\"Role title inferred from the JD\")\n",
        "    seniority: str = Field(..., description=\"Seniority level (e.g., Junior/Mid/Senior/Lead)\")\n",
        "    must_have_skills: List[str]\n",
        "    nice_to_have_skills: List[str]\n",
        "    responsibilities: List[str]\n",
        "    interview_focus_areas: List[str] = Field(..., description=\"Competency areas to cover\")\n",
        "    evaluation_rubric: List[str] = Field(..., description=\"Scoring guidelines\")\n",
        "\n",
        "class InterviewQuestion(BaseModel):\n",
        "    id: str\n",
        "    competency: str\n",
        "    difficulty: Literal[\"easy\", \"medium\", \"hard\"]\n",
        "    question: str\n",
        "    expected_signals: List[str]\n",
        "    red_flags: List[str]\n",
        "\n",
        "class QuestionSet(BaseModel):\n",
        "    questions: List[InterviewQuestion]\n",
        "\n",
        "class AnswerEvaluation(BaseModel):\n",
        "    question_id: str\n",
        "    score_1_to_5: int = Field(..., ge=1, le=5)\n",
        "    rationale: str\n",
        "    signal_tags: List[str] = Field(default_factory=list)\n",
        "    followup_question: Optional[str] = None\n",
        "\n",
        "class HiringRecommendation(BaseModel):\n",
        "    decision: Literal[\"Hire\", \"Hold\", \"No Hire\"]\n",
        "    overall_score_1_to_5: float\n",
        "    strengths: List[str]\n",
        "    gaps: List[str]\n",
        "    risks: List[str]\n",
        "    suggested_next_steps: List[str]\n",
        "    summary: str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create agents (LangChain + AzureChatOpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    azure_deployment=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "jd_analyzer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an HR tech assistant. Extract structured hiring requirements from the job description. Return only JSON.\"),\n",
        "    (\"human\", \"JOB DESCRIPTION: {jd}\")\n",
        "])\n",
        "\n",
        "question_gen_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an interview designer. Create a balanced set of interview questions covering the focus areas. Return only JSON.\"),\n",
        "    (\"human\", \"JOB ANALYSIS (JSON): {analysis_json} Generate {n_questions} questions.\")\n",
        "])\n",
        "\n",
        "evaluator_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an interview evaluator. Score the candidate answer using the rubric. Return only JSON.\"),\n",
        "    (\"human\", \"RUBRIC: {rubric} QUESTION: {question} EXPECTED SIGNALS:{signals} RED FLAGS: {red_flags} CANDIDATE ANSWER:{answer}\")\n",
        "])\n",
        "\n",
        "recommender_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a hiring committee chair. Synthesize evaluations into a final recommendation. Return only JSON.\"),\n",
        "    (\"human\", \"JOB ANALYSIS (JSON): {analysis_json} EVALUATIONS (JSON): {evaluations_json}\")\n",
        "])\n",
        "\n",
        "jd_analyzer = jd_analyzer_prompt | llm.with_structured_output(JobAnalysis)\n",
        "question_generator = question_gen_prompt | llm.with_structured_output(QuestionSet)\n",
        "answer_evaluator = evaluator_prompt | llm.with_structured_output(AnswerEvaluation)\n",
        "recommender = recommender_prompt | llm.with_structured_output(HiringRecommendation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Orchestration with LangGraph (multi-agent workflow + session memory)\n",
        "\n",
        "Workflow:\n",
        "\n",
        "`Analyze JD → Generate Questions → (Pick Q → Evaluate A)* → Recommend`\n",
        "\n",
        "A **checkpointer** stores state per `thread_id` (session), enabling multi-turn memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import Any, Dict, List\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "class InterviewState(TypedDict, total=False):\n",
        "    jd: str\n",
        "    analysis: Dict[str, Any]\n",
        "    questions: List[Dict[str, Any]]\n",
        "    candidate_answers: List[str]\n",
        "    evaluations: List[Dict[str, Any]]\n",
        "    current_index: int\n",
        "    current_question: Dict[str, Any]\n",
        "    recommendation: Dict[str, Any]\n",
        "\n",
        "\n",
        "def node_analyze_jd(state: InterviewState):\n",
        "    analysis = jd_analyzer.invoke({\"jd\": state[\"jd\"]}).model_dump()\n",
        "    return {\"analysis\": analysis}\n",
        "\n",
        "\n",
        "def node_generate_questions(state: InterviewState):\n",
        "    qset = question_generator.invoke({\n",
        "        \"analysis_json\": json.dumps(state[\"analysis\"], ensure_ascii=False, indent=2),\n",
        "        \"n_questions\": 8,\n",
        "    }).model_dump()\n",
        "    return {\"questions\": list(qset[\"questions\"]), \"current_index\": 0, \"evaluations\": []}\n",
        "\n",
        "\n",
        "def node_pick_question(state: InterviewState):\n",
        "    idx = state.get(\"current_index\", 0)\n",
        "    return {\"current_question\": state[\"questions\"][idx]}\n",
        "\n",
        "\n",
        "def node_evaluate_answer(state: InterviewState):\n",
        "    idx = state[\"current_index\"]\n",
        "    q = state[\"current_question\"]\n",
        "    answer = state[\"candidate_answers\"][idx] if idx < len(state.get(\"candidate_answers\", [])) else \"\"\n",
        "\n",
        "    rubric = \"\".join(state[\"analysis\"].get(\"evaluation_rubric\", []))\n",
        "\n",
        "    ev = answer_evaluator.invoke({\n",
        "        \"rubric\": rubric,\n",
        "        \"question\": q[\"question\"],\n",
        "        \"signals\": \"\".join(q.get(\"expected_signals\", [])),\n",
        "        \"red_flags\": \"\".join(q.get(\"red_flags\", [])),\n",
        "        \"answer\": answer,\n",
        "    }).model_dump()\n",
        "\n",
        "    ev[\"question_id\"] = q[\"id\"]\n",
        "    return {\"evaluations\": state.get(\"evaluations\", []) + [ev], \"current_index\": idx + 1}\n",
        "\n",
        "\n",
        "def should_continue(state: InterviewState):\n",
        "    idx = state.get(\"current_index\", 0)\n",
        "    if idx < len(state.get(\"questions\", [])) and idx < len(state.get(\"candidate_answers\", [])):\n",
        "        return \"continue\"\n",
        "    return \"recommend\"\n",
        "\n",
        "\n",
        "def node_recommend(state: InterviewState):\n",
        "    rec = recommender.invoke({\n",
        "        \"analysis_json\": json.dumps(state[\"analysis\"], ensure_ascii=False, indent=2),\n",
        "        \"evaluations_json\": json.dumps(state.get(\"evaluations\", []), ensure_ascii=False, indent=2),\n",
        "    }).model_dump()\n",
        "    return {\"recommendation\": rec}\n",
        "\n",
        "\n",
        "builder = StateGraph(InterviewState)\n",
        "builder.add_node(\"analyze_jd\", node_analyze_jd)\n",
        "builder.add_node(\"generate_questions\", node_generate_questions)\n",
        "builder.add_node(\"pick_question\", node_pick_question)\n",
        "builder.add_node(\"evaluate_answer\", node_evaluate_answer)\n",
        "builder.add_node(\"recommend\", node_recommend)\n",
        "\n",
        "builder.add_edge(START, \"analyze_jd\")\n",
        "builder.add_edge(\"analyze_jd\", \"generate_questions\")\n",
        "builder.add_edge(\"generate_questions\", \"pick_question\")\n",
        "builder.add_edge(\"pick_question\", \"evaluate_answer\")\n",
        "\n",
        "builder.add_conditional_edges(\"evaluate_answer\", should_continue, {\n",
        "    \"continue\": \"pick_question\",\n",
        "    \"recommend\": \"recommend\",\n",
        "})\n",
        "\n",
        "builder.add_edge(\"recommend\", END)\n",
        "\n",
        "checkpointer = MemorySaver()\n",
        "interview_graph = builder.compile(checkpointer=checkpointer)\n",
        "print(\"Graph compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Demo run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_jd = '''\n",
        "Role: Senior Data Engineer\n",
        "Responsibilities:\n",
        "- Build and maintain data pipelines (batch and streaming) using Python and Spark\n",
        "- Design data models in a cloud data warehouse (Azure Synapse / Databricks / Snowflake)\n",
        "- Implement CI/CD, testing, and monitoring for data workflows\n",
        "- Collaborate with analysts and data scientists to deliver curated datasets\n",
        "Requirements:\n",
        "- 5+ years in data engineering\n",
        "- Strong Python, SQL, Spark\n",
        "- Experience with Azure (ADF, Databricks) preferred\n",
        "- Knowledge of data governance and security\n",
        "'''.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8628d15e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# 1) Analyze JD\n",
        "analysis_obj = jd_analyzer.invoke({\"jd\": sample_jd})\n",
        "analysis = analysis_obj.model_dump()\n",
        "\n",
        "# 2) Generate Questions\n",
        "qset_obj = question_generator.invoke({\n",
        "    \"analysis_json\": json.dumps(analysis, ensure_ascii=False, indent=2),\n",
        "    \"n_questions\": 3\n",
        "})\n",
        "questions = [q.model_dump() if hasattr(q, \"model_dump\") else q for q in qset_obj.questions]\n",
        "\n",
        "# 3) Collect answers interactively\n",
        "answers = []\n",
        "for q in questions:\n",
        "    print(\"\\nQUESTION:\", q[\"question\"])\n",
        "    ans = input(\"Your answer: \")\n",
        "    answers.append(ans)\n",
        "\n",
        "# 4) Evaluate answers\n",
        "evaluations = []\n",
        "rubric = \"\\n\".join(analysis.get(\"evaluation_rubric\", []))\n",
        "\n",
        "for q, ans in zip(questions, answers):\n",
        "    ev = answer_evaluator.invoke({\n",
        "        \"rubric\": rubric,\n",
        "        \"question\": q[\"question\"],\n",
        "        \"signals\": \"\\n\".join(q.get(\"expected_signals\", [])),\n",
        "        \"red_flags\": \"\\n\".join(q.get(\"red_flags\", [])),\n",
        "        \"answer\": ans\n",
        "    }).model_dump()\n",
        "    ev[\"question_id\"] = q[\"id\"]\n",
        "    evaluations.append(ev)\n",
        "\n",
        "# 5) Final recommendation\n",
        "rec = recommender.invoke({\n",
        "    \"analysis_json\": json.dumps(analysis, ensure_ascii=False, indent=2),\n",
        "    \"evaluations_json\": json.dumps(evaluations, ensure_ascii=False, indent=2)\n",
        "}).model_dump()\n",
        "\n",
        "print(\"\\n=== FINAL RECOMMENDATION ===\")\n",
        "print(json.dumps(rec, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Optional: Load job description from Azure Blob Storage\n",
        "\n",
        "Store job descriptions and transcripts in Blob Storage if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional helper (not executed by default)\n",
        "# from azure.storage.blob import BlobServiceClient\n",
        "#\n",
        "# def load_text_from_blob(conn_str: str, container: str, blob_name: str) -> str:\n",
        "#     bsc = BlobServiceClient.from_connection_string(conn_str)\n",
        "#     blob_client = bsc.get_blob_client(container=container, blob=blob_name)\n",
        "#     return blob_client.download_blob().readall().decode('utf-8')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
